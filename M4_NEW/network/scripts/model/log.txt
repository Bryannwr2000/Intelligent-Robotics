Net Architecture:
Resnet18Skip(
  (res18_backbone): Sequential(
    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  )
  (conv2_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv3_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv4_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv5_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (top_conv): Sequential(
    (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (lateral_conv1): Sequential(
    (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (lateral_conv2): Sequential(
    (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (lateral_conv3): Sequential(
    (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (segmentation_conv): Sequential(
    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): Conv2d(64, 6, kernel_size=(1, 1), stride=(1, 1))
  )
  (criterion): CrossEntropyLoss()
)
Loss Function: CrossEntropyLoss

===========================================================
==================== Hyper-parameters =====================
n_classes: 5
lr: 0.001
epochs: 40
batch_size: 64
weight_decay: 0.0001
scheduler_step: 5
scheduler_gamma: 0.5
model_dir: model
load_best: 0
log_freq: 20
dataset_dir: dataset
===========================================================
============= Epoch 0 | 2022-09-13 00:49:35 ===============
=> Current Lr: 0.001
[0/18]: 2.2374
=> Training Loss: 0.3675, Evaluation Loss 0.2619

============= Epoch 1 | 2022-09-13 00:53:33 ===============
=> Current Lr: 0.001
[0/18]: 0.1616
=> Training Loss: 0.1305, Evaluation Loss 0.0927

============= Epoch 2 | 2022-09-13 00:57:30 ===============
=> Current Lr: 0.001
[0/18]: 0.1016
=> Training Loss: 0.0785, Evaluation Loss 0.0717

============= Epoch 3 | 2022-09-13 01:01:24 ===============
=> Current Lr: 0.001
[0/18]: 0.0631
=> Training Loss: 0.0623, Evaluation Loss 0.0702

============= Epoch 4 | 2022-09-13 01:05:24 ===============
=> Current Lr: 0.001
[0/18]: 0.0566
=> Training Loss: 0.0544, Evaluation Loss 0.0488

============= Epoch 5 | 2022-09-13 01:09:23 ===============
=> Current Lr: 0.0005
[0/18]: 0.0484
=> Training Loss: 0.0468, Evaluation Loss 0.0479

============= Epoch 6 | 2022-09-13 01:13:18 ===============
=> Current Lr: 0.0005
[0/18]: 0.0428
=> Training Loss: 0.0440, Evaluation Loss 0.0454

============= Epoch 7 | 2022-09-13 01:17:14 ===============
=> Current Lr: 0.0005
[0/18]: 0.0413
=> Training Loss: 0.0431, Evaluation Loss 0.0466

============= Epoch 8 | 2022-09-13 01:21:11 ===============
=> Current Lr: 0.0005
[0/18]: 0.0472
=> Training Loss: 0.0423, Evaluation Loss 0.0433

============= Epoch 9 | 2022-09-13 01:25:07 ===============
=> Current Lr: 0.0005
[0/18]: 0.0376
=> Training Loss: 0.0397, Evaluation Loss 0.0393

============= Epoch 10 | 2022-09-13 01:29:02 ==============
=> Current Lr: 0.00025
[0/18]: 0.0452
=> Training Loss: 0.0367, Evaluation Loss 0.0349

============= Epoch 11 | 2022-09-13 01:32:59 ==============
=> Current Lr: 0.00025
[0/18]: 0.0352
=> Training Loss: 0.0323, Evaluation Loss 0.0296

============= Epoch 12 | 2022-09-13 01:36:53 ==============
=> Current Lr: 0.00025
[0/18]: 0.0230
=> Training Loss: 0.0289, Evaluation Loss 0.0292

============= Epoch 13 | 2022-09-13 01:40:48 ==============
=> Current Lr: 0.00025
[0/18]: 0.0229
=> Training Loss: 0.0275, Evaluation Loss 0.0297

============= Epoch 14 | 2022-09-13 01:44:43 ==============
=> Current Lr: 0.00025
[0/18]: 0.0320
=> Training Loss: 0.0317, Evaluation Loss 0.0308

============= Epoch 15 | 2022-09-13 01:48:37 ==============
=> Current Lr: 0.000125
[0/18]: 0.0292
=> Training Loss: 0.0293, Evaluation Loss 0.0267

============= Epoch 16 | 2022-09-13 01:52:32 ==============
=> Current Lr: 0.000125
[0/18]: 0.0364
=> Training Loss: 0.0251, Evaluation Loss 0.0224

============= Epoch 17 | 2022-09-13 01:56:26 ==============
=> Current Lr: 0.000125
[0/18]: 0.0211
=> Training Loss: 0.0213, Evaluation Loss 0.0182

============= Epoch 18 | 2022-09-13 02:00:20 ==============
=> Current Lr: 0.000125
[0/18]: 0.0181
=> Training Loss: 0.0203, Evaluation Loss 0.0374

============= Epoch 19 | 2022-09-13 02:04:14 ==============
=> Current Lr: 0.000125
[0/18]: 0.0244
=> Training Loss: 0.0220, Evaluation Loss 0.0195

============= Epoch 20 | 2022-09-13 02:08:08 ==============
=> Current Lr: 6.25e-05
[0/18]: 0.0228
=> Training Loss: 0.0201, Evaluation Loss 0.0185

============= Epoch 21 | 2022-09-13 02:12:02 ==============
=> Current Lr: 6.25e-05
[0/18]: 0.0156
=> Training Loss: 0.0182, Evaluation Loss 0.0149

============= Epoch 22 | 2022-09-13 02:15:55 ==============
=> Current Lr: 6.25e-05
[0/18]: 0.0180
=> Training Loss: 0.0174, Evaluation Loss 0.0137

============= Epoch 23 | 2022-09-13 02:19:50 ==============
=> Current Lr: 6.25e-05
[0/18]: 0.0120
=> Training Loss: 0.0163, Evaluation Loss 0.0198

============= Epoch 24 | 2022-09-13 02:23:45 ==============
=> Current Lr: 6.25e-05
[0/18]: 0.0212
=> Training Loss: 0.0153, Evaluation Loss 0.0246

============= Epoch 25 | 2022-09-13 02:27:39 ==============
=> Current Lr: 3.125e-05
[0/18]: 0.0180
=> Training Loss: 0.0165, Evaluation Loss 0.0136

============= Epoch 26 | 2022-09-13 02:31:32 ==============
=> Current Lr: 3.125e-05
[0/18]: 0.0169
=> Training Loss: 0.0137, Evaluation Loss 0.0118

============= Epoch 27 | 2022-09-13 02:35:24 ==============
=> Current Lr: 3.125e-05
[0/18]: 0.0198
=> Training Loss: 0.0132, Evaluation Loss 0.0138

============= Epoch 28 | 2022-09-13 02:39:18 ==============
=> Current Lr: 3.125e-05
[0/18]: 0.0125
=> Training Loss: 0.0145, Evaluation Loss 0.0251

============= Epoch 29 | 2022-09-13 02:43:11 ==============
=> Current Lr: 3.125e-05
[0/18]: 0.0265
=> Training Loss: 0.0138, Evaluation Loss 0.0121

============= Epoch 30 | 2022-09-13 02:47:05 ==============
=> Current Lr: 1.5625e-05
[0/18]: 0.0092
=> Training Loss: 0.0125, Evaluation Loss 0.0107

============= Epoch 31 | 2022-09-13 02:51:00 ==============
=> Current Lr: 1.5625e-05
[0/18]: 0.0126
=> Training Loss: 0.0126, Evaluation Loss 0.0133

============= Epoch 32 | 2022-09-13 03:56:03 ==============
=> Current Lr: 1.5625e-05
[0/18]: 0.0112
=> Training Loss: 0.0120, Evaluation Loss 0.0119

============= Epoch 33 | 2022-09-13 03:59:58 ==============
=> Current Lr: 1.5625e-05
[0/18]: 0.0109
=> Training Loss: 0.0117, Evaluation Loss 0.0142

============= Epoch 34 | 2022-09-13 04:03:52 ==============
=> Current Lr: 1.5625e-05
[0/18]: 0.0105
=> Training Loss: 0.0111, Evaluation Loss 0.0115

============= Epoch 35 | 2022-09-13 04:07:47 ==============
=> Current Lr: 7.8125e-06
[0/18]: 0.0108
=> Training Loss: 0.0114, Evaluation Loss 0.0117

============= Epoch 36 | 2022-09-13 04:11:41 ==============
=> Current Lr: 7.8125e-06
[0/18]: 0.0146
=> Training Loss: 0.0112, Evaluation Loss 0.0111

============= Epoch 37 | 2022-09-13 04:15:35 ==============
=> Current Lr: 7.8125e-06
[0/18]: 0.0090
=> Training Loss: 0.0106, Evaluation Loss 0.0099

============= Epoch 38 | 2022-09-13 04:19:35 ==============
=> Current Lr: 7.8125e-06
[0/18]: 0.0138
=> Training Loss: 0.0116, Evaluation Loss 0.0107

============= Epoch 39 | 2022-09-13 04:23:30 ==============
=> Current Lr: 7.8125e-06
[0/18]: 0.0114
=> Training Loss: 0.0116, Evaluation Loss 0.0109
